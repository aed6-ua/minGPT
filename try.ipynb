{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcasa\\anaconda3\\envs\\nlpt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.bpe import BPETokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTWithEmbeddings(GPT):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        super().__init__(config)  # Call the constructor of the parent class (GPT)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"\n",
    "        Initialize a pretrained GPT model by copying over the weights\n",
    "        from a huggingface/transformers checkpoint.\n",
    "        \"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = cls.get_default_config()\n",
    "        config.model_type = model_type\n",
    "        config.vocab_size = 50257 # openai's model vocabulary\n",
    "        config.block_size = 1024  # openai's model block_size\n",
    "        model = cls(config)\n",
    "        sd = model.state_dict()\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(keys) == len([k for k in sd if not k.endswith(\".attn.bias\")])\n",
    "        for k in keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def forward(self, idx, targets=None, embeddings_input=None):\n",
    "      device = idx.device if idx is not None else embeddings_input.device\n",
    "      if idx is not None:\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "      else:\n",
    "        print(\"Embeddings input shape:\", embeddings_input.shape)\n",
    "        x = embeddings_input\n",
    "        b, t, _ = x.shape\n",
    "      \n",
    "      all_embeddings = [x.detach().cpu()] # Store initial embeddings\n",
    "      \n",
    "      x = self.transformer.drop(x)\n",
    "      for block in self.transformer.h:\n",
    "          x = block(x)\n",
    "          all_embeddings.append(x.detach().cpu()) # Store embeddings after each block\n",
    "      x = self.transformer.ln_f(x)\n",
    "      all_embeddings.append(x.detach().cpu()) # Store embeddings after layer norm\n",
    "\n",
    "      logits = self.lm_head(x)\n",
    "\n",
    "      # if we are given some desired targets also calculate the loss\n",
    "      loss = None\n",
    "      if targets is not None:\n",
    "          loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "      return logits, loss, torch.stack(all_embeddings, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT.from_pretrained('gpt2').to(device)\n",
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clean and corrupted inputs\n",
    "clean_text = \"Michelle Jones was a top-notch student. Michelle\"\n",
    "corrupted_text = \"Michelle Smith was a top-notch student. Michelle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first logits shape = torch.Size([1, 14, 50257])\n",
      "model activations last =  torch.Size([1, 14, 768])\n",
      "probs shape =  torch.Size([1, 14, 50257])\n",
      "torch.Size([20])\n",
      "Top 20 tokens at last position:\n",
      " m:\t\t 10.1117 %\n",
      " de:\t\t 4.1587 %\n",
      " es:\t\t 3.6096 %\n",
      " put:\t\t 3.3308 %\n",
      ",:\t\t 3.2863 %\n",
      " s:\t\t 1.9430 %\n",
      ".:\t\t 1.7038 %\n",
      " la:\t\t 1.2985 %\n",
      " ma:\t\t 1.2802 %\n",
      " en:\t\t 1.2744 %\n",
      " d:\t\t 1.1636 %\n",
      " el:\t\t 1.0951 %\n",
      " se:\t\t 1.0758 %\n",
      " est:\t\t 0.9695 %\n",
      " que:\t\t 0.9054 %\n",
      " si:\t\t 0.8046 %\n",
      " al:\t\t 0.8030 %\n",
      " n:\t\t 0.7923 %\n",
      " a:\t\t 0.7753 %\n",
      " mad:\t\t 0.7253 %\n",
      "Tu puta madre es muy puta. Tu madre es\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the clean text\n",
    "clean_tokens = tokenizer(clean_text)[0].unsqueeze(0).to(device)\n",
    "corrupted_tokens = tokenizer(corrupted_text)[0].unsqueeze(0).to(device)\n",
    "\n",
    "# Pass it through the model via the generate function\n",
    "output = model.generate(clean_tokens, max_new_tokens=1,temperature=0.001, do_sample=False)\n",
    "\n",
    "first_logits = model(clean_tokens)\n",
    "print(\"first logits shape =\" , first_logits[0].shape)\n",
    "# Display the most probable continuations based on the logits of the last token\n",
    "logits = model.activations[-1][0, -1]\n",
    "probs = F.softmax(first_logits[0], dim=-1)\n",
    "print(\"model activations last = \", model.activations[-1].shape)\n",
    "print(\"probs shape = \", probs.shape)\n",
    "top_probs, top_indices = torch.topk(probs, k=20, dim=-1)\n",
    "print(top_indices[0][-1].shape)\n",
    "\n",
    "top_tokens = [tokenizer.decode(torch.tensor([idx])) for idx in top_indices[0][-1]]\n",
    "\n",
    "print(\"Top 20 tokens at last position:\")\n",
    "for prob, token in zip(top_probs[0][-1], top_tokens):\n",
    "    print(f\"{token}:\\t\\t {prob.item()*100:.4f} %\")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m patched_tokens \u001b[38;5;241m=\u001b[39m corrupted_tokens\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Generate output with patched tokens\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatched_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_activations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Compute logits for the last token\u001b[39;00m\n\u001b[0;32m     13\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlast_token_logits\n",
      "File \u001b[1;32mc:\\Users\\mcasa\\Documents\\UA\\NLPT\\minGPT\\mingpt\\model.py:262\u001b[0m, in \u001b[0;36mGPT.forward\u001b[1;34m(self, idx, targets, save_activations, patch_embedding)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, save_activations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, patch_embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    261\u001b[0m     device \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 262\u001b[0m     b, t \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot forward sequence of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, block size is only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m     pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, t, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# shape (1, t)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Initialize matrix to store logit differences\n",
    "logit_diff_matrix = torch.zeros((12 + 1, len(corrupted_tokens)))\n",
    "\n",
    "# Iterate through layers and token positions\n",
    "for layer in range(12 + 1):\n",
    "    for pos in range(len(corrupted_tokens)):\n",
    "        patched_tokens = corrupted_tokens.clone()\n",
    "\n",
    "        # Generate output with patched tokens\n",
    "        logits, _ = model.forward(patched_tokens.unsqueeze(0).to(device), save_activations=True, patch_embedding={'layer': layer, 'position': pos, 'embedding': model.transformer.wte(clean_tokens[pos].unsqueeze(0).to(device))})\n",
    "\n",
    "        # Compute logits for the last token\n",
    "        logits = model.last_token_logits\n",
    "        \n",
    "        # Calculate logit difference between ' Smith' and ' Jones'\n",
    "        logit_diff = logits[0, smith_index] - logits[0, jones_index]\n",
    "\n",
    "        # Store logit difference in the matrix\n",
    "        logit_diff_matrix[layer, pos] = logit_diff.item()\n",
    "\n",
    "# Print the logit difference matrix\n",
    "print(\"Logit Difference Matrix:\")\n",
    "print(logit_diff_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
