{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcasa\\anaconda3\\envs\\nlpt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import BPETokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Michelle Jones was a top-notch student. Michelle\n",
      "Tokenized input: tensor([48736,  5437,   373,   257,  1353,    12,  1662,   354,  3710,    13,\n",
      "        16738])\n",
      "Number of input tokens: 11\n",
      "Detokenized input from indices: Michelle Jones was a top-notch student. Michelle\n",
      "Detokenized input as strings: Michelle/ Jones/ was/ a/ top/-/not/ch/ student/./ Michelle\n"
     ]
    }
   ],
   "source": [
    "input = \"Michelle Jones was a top-notch student. Michelle\"\n",
    "print(\"Input:\", input)\n",
    "bpe = BPETokenizer()\n",
    "# bpe() gets a string and returns a 2D batch tensor \n",
    "# of indices with shape (1, input_length)\n",
    "tokens = bpe(input)[0]\n",
    "print(\"Tokenized input:\", tokens)\n",
    "input_length = tokens.shape[-1]\n",
    "print(\"Number of input tokens:\", input_length)\n",
    "# bpe.decode gets a 1D tensor (list of indices) and returns a string\n",
    "print(\"Detokenized input from indices:\", bpe.decode(tokens))  \n",
    "tokens_str = []\n",
    "for token in tokens:\n",
    "    decoded_token = bpe.decode(torch.tensor([token]))\n",
    "    tokens_str.append(decoded_token)\n",
    "print(\"Detokenized input as strings: \" + '/'.join(tokens_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt2-xl'\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained(model)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "use_minigpt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "\n",
    "\n",
    "# tokenize the input prompt into integer input sequence\n",
    "    tokenizer = BPETokenizer()\n",
    "    if prompt == '':\n",
    "        # to create unconditional samples...\n",
    "        # manually create a tensor with only the special <|endoftext|> token\n",
    "        # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt).to(device)\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    # Write outputs to file\n",
    "    with open('output.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('Prompt: ' + prompt + '\\n\\n')\n",
    "        for i in range(num_samples):\n",
    "            out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "            print('-'*80)\n",
    "            print(out)\n",
    "            f.write(out + '\\n')  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle had a top 10 average and had participated in numerous extracurricular activities. She had a \"\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle's parents believed that they had everything a woman could want. She was a varsity athlete and a\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle was also a victim of domestic violence,\" said CPD spokeswoman Melissa Matey.\n",
      "\n",
      "Police said\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle had won all seven Advanced Placement classes she had taken in high school. She had been a member\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle, who was born with severe spina bifida and cerebral palsy, was enrolled at the\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle Jones is a student of the world. Now, in this very small world, our lives hang in\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle had taken a few classes when she was younger and was studying for her GED. She also went\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle was a star soccer player, and she was the class valedictorian of her high school class\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle Jones was a very good friend as well as a wonderful student. Michelle Jones has no criminal record.\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Jones was a top-notch student. Michelle and her twin sisters were on the honor roll and active in their church. She was very popular with\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle was studying English. She was the daughter of a judge and she was a straight A student.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle's father was a retired Army colonel, and the family lived in Washington, D.C., where\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle left the UK to travel and study in France. There, after a year, she joined a revolutionary\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle took a job as an account coordinator with Chase Bank â€” she graduated from the college with plans to become\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle Smith\n",
      "\n",
      "That day in November 1997 went down in school lore like a scene from a horror movie\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle received first-class honors, received an offer to play basketball and was the MVP of the high school\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle Smith, a 17-year-old from New Castle, Delaware, was a high school student on\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle came from a very humble background. I guess, it wasn't as simple as that (laugh).\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle had won a scholarship to the University of Kentucky, and her grades were outstanding. She was the most\n",
      "--------------------------------------------------------------------------------\n",
      "Michelle Smith was a top-notch student. Michelle's grades were better than her husband's, who was in a failing high school. Her husband was\n"
     ]
    }
   ],
   "source": [
    "inputs =['Michelle Jones was a top-notch student. Michelle',\n",
    "         'Michelle Smith was a top-notch student. Michelle',\n",
    "         'Jessica Jones was a top-notch student. Michelle',\n",
    "         'Michelle Smith was a top-notch student. Jessica']\n",
    "\n",
    "for input in inputs:\n",
    "    generate(prompt=input, num_samples=10, steps=20, do_sample=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
